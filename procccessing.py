# ðŸ“Œ GEREKLÄ° KÃœTÃœPHANELER
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer

# ðŸ“Œ NLTK VERÄ°LERÄ°NÄ° Ä°NDÄ°R (ilk seferde gerekir)
nltk.download('stopwords')
nltk.download('wordnet')

# ðŸ“Œ 1. DOSYAYI OKU
# ADMISSIONS.csv iÃ§inden 'diagnosis' sÃ¼tununu al
df = pd.read_csv("ADMISSIONS.csv")
diagnoses = df['diagnosis'].dropna().astype(str)

# ðŸ“Œ 2. ARAÃ‡LARI HAZIRLA
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

# ðŸ“Œ 3. Ã–N Ä°ÅžLEME: Temizleme, KÃ¼Ã§Ã¼k Harfe Ã‡evirme, Tokenizasyon, Stop Word, Lemma, Stem
processed_texts = []

for text in diagnoses:
    # Lowercasing
    text = text.lower()
    
    # HTML ve Ã¶zel karakter temizliÄŸi
    text = re.sub(r'<[^>]+>', '', text)               # HTML etiketlerini temizle
    text = re.sub(r'[^a-z\s]', '', text)              # Noktalama ve Ã¶zel karakter temizliÄŸi
    
    # Tokenization (split ile)
    tokens = text.split()
    
    # Stop word removal
    tokens = [word for word in tokens if word not in stop_words]
    
    # Lemmatization
    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]
    
    # Stemming
    stemmed = [stemmer.stem(word) for word in lemmatized]
    
    # Temiz metni birleÅŸtir
    final_text = " ".join(stemmed)
    processed_texts.append(final_text)

# ðŸ“Œ 4. YENÄ° DATAFRAME OLUÅžTUR
output_df = pd.DataFrame({
    "original_text": diagnoses.values,
    "cleaned_text": processed_texts
})

# ðŸ“Œ 5. CSV DOSYASINA KAYDET
output_df.to_csv("cleaned_diagnosis_texts.csv", index=False)

print("TemizlenmiÅŸ veriler 'cleaned_diagnosis_texts.csv' olarak kaydedildi.")
